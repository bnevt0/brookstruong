---
title: "AT2 Credit Default"
author: "Edward Truong / Alex Brooks"
date: "26/09/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objectives
1. Develop and deploy a classification model an a product purchase data set.
2. End to end analysis using R
3. Learn the `caret` package for ML
4. Learn to present the case using Rmarkdown

## Read in the dataset

```{r Read in the dataset, error=FALSE, message=FALSE, warning = FALSE}
library(tidyverse, quietly = T)
library(dplyr, quietly = T)      #used by Caret
library(ggplot2, quietly = T)
library(corrplot, quietly =T)
library(caret, quietly = T)
library(Amelia, quietly = T)
library(gridExtra, quietly = T)
library(xgboost, quietly = T)
library(ROCR, quietly = T)
library(Matrix, quietly = T)

setwd("~/Documents/brookstruong")

#Import the data
training.raw <- read.csv('AT2_credit_train_STUDENT.csv', header = TRUE)
predict.raw <- read.csv('AT2_credit_test_STUDENT.csv', header = TRUE)

```

23,101 observations over 17 variables

##Clean the data based on new observations to make train and test the same

```{r data_clean}
#Data cleaning function we can use each time to make sure we share the same dataset  
#Cleaned ID into character, Sex into two variables (to remove animals), Age into NAs for over 100 errored data and then transformed those NAs into median age of 35, Education into 4 categorised variables, marriage into 3 categorised variables. See explanation in markdown if you want to know more.
#also removed previously coded factors of marriage, sex and education

clean_data <- function(dataSet) {
  
  #clean up sex to remove the animal classifications 
  output <- dataSet
  
  output$ID <- as.character(output$ID)
  
  output$SEX <- as.integer(output$SEX)
  
  output$SEX[output$SEX > 2] <- 0

   #all education factors greater than 4, set them to 4, also set 0 to 4
  output$EDUCATION[output$EDUCATION > 4] <- 4
  output$EDUCATION[output$EDUCATION == 0] <- 4
  #clean marriage
   output$MARRIAGE[output$MARRIAGE == 0] <- 3

  
  return(output)
}

```



```{r Trainset, echo=TRUE}
glimpse(train.raw)

train.raw$default <- as.factor(train.raw$default)
train.raw$SEX <- as.factor(train.raw$SEX)
train.raw$EDUCATION <- as.factor(train.raw$EDUCATION)
train.raw$MARRIAGE <- as.factor(train.raw$MARRIAGE)

summary(train.raw)
```

Looking at the dataset and how the `read_csv` imported the csv, we can understand that the date is imported as a <chr> string rather than <date>. 

```{r, echo=TRUE}
train.raw %>%
  arrange(AGE>80) %>%
  ggplot(aes(x=AGE)) + geom_histogram() 

ggplot

```

Missing data = 3NA for sex.

```{r Correlation Plot}
cplot <- train.raw %>%
  select(-default) %>%
  select_if(is.numeric)

M <- cor(cplot)
p.mat <- cor.mtest(M)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, 
         method = "color",
         order= "hclust",
         type="full",
         col=col(200),
         diag =F,
         title="Correlation of Numeric Variables",
         addCoef.col = "black",
         sig.level = 0.05,
         insig ="blank",
         mar=c(0,0,3,0))

```

```{r}
train.raw %>%
  filter(SEX == "NA")

```

## Train-Test Split

```{r Train-Test Split, echo=TRUE}
set.seed(42)
training_rows <- createDataPartition(y = train.raw$default, p=0.7, list=F)

test.raw <- train.raw %>% filter(!(rownames(.) %in% training_rows))
train.raw <- train.raw %>% filter(rownames(.) %in% training_rows)
dim(train.raw)

```

```{r pressure, echo=FALSE}
dim(test.raw)
```

## Missing values analysis

```{r}

missmap(train.raw, main='Missing Values Analysis using Amelia ordered by % missing', col=c('red', 'gray'), legend =F, rank.order = T)


```

```{r}

map_int(train.raw,~sum(is.na(.x)))


```

Sex is missing 2 values. 

## EDA

`default` is the response variable. The response variable is a Yes/No boolean variable therefor is appropriate for our classification problem. 

```{r}

round(prop.table(table(train.raw$default)),2)


```

76% of the dataset do not default and 24% have defaulted.

***

## Predictor Variables

### Univariate & Bivariate

First step is to look at all variables available using the `ggplot2` framework for visuals.

#### Continuous Variables

1. `LIMIT_BAL` The limit Balance beings at -99 with a median of 140,000, mean of 167880 and max of 1,000,000.
2. `AGE` certainly shows many outliers beyond the 100+ range. Age begins at 21 with a median of 34, mean of 35.65 and max of 141.

```{r}
p1 <- ggplot(data=train.raw, aes(x=LIMIT_BAL)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p2 <- ggplot(data=train.raw, aes(x=AGE)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
grid.arrange(p1,p2, nrow=1)
```

```{r}
p3 <- ggplot(data=train.raw, aes(x=PAY_PC1)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p4 <- ggplot(data=train.raw, aes(x=PAY_PC2)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p5 <- ggplot(data=train.raw, aes(x=PAY_PC3)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
grid.arrange(p3,p4,p5, nrow=1)
```


```{r}
p6 <- ggplot(data=train.raw, aes(x=AMT_PC1)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p7 <- ggplot(data=train.raw, aes(x=AMT_PC2)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p8 <- ggplot(data=train.raw, aes(x=AMT_PC3)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p9 <- ggplot(data=train.raw, aes(x=AMT_PC4)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p10 <- ggplot(data=train.raw, aes(x=AMT_PC5)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p11 <- ggplot(data=train.raw, aes(x=AMT_PC6)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p12 <- ggplot(data=train.raw, aes(x=AMT_PC7)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
grid.arrange(p6,p7,p8,p9,p10,p11,p12, nrow=2)
```
#### Categorical Variables

1. `SEX` The limit Balance beings at -99 with a median of 140,000, mean of 167880 and max of 1,000,000.
2. `EDUCATION` certainly shows many outliers beyond the 100+ range. Age begins at 21 with a median of 34, mean of 35.65 and max of 141.
2. `MARRIAGE` certainly shows many outliers beyond the 100+ range. Age begins at 21 with a median of 34, mean of 35.65 and max of 141.

```{r}

get_legend<-function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
p <- lapply(X = c('SEX', 'EDUCATION', 'MARRIAGE'),
            FUN = function(x) ggplot(data = train.raw) +
              aes_string(x=x, fill = 'default') +
              geom_bar(position="dodge") +
              theme(legend.position="none"))
legend <- get_legend(ggplot(data = train.raw, aes(x=SEX, fill = default)) +
                       geom_bar())

grid.arrange(p[[1]],p[[2]],p[[3]],
             legend, layout_matrix = cbind(c(1,2,3),
                                           c(4,5,3),
                                           c(6,6,6)),
             widths=c(3,3,1))

```

***
## Data Preparation

### Missing Values Imputation
```{r, }

train.imp <- train.raw[complete.cases(train.raw), ]
```

#### Age Imputation.

```{r}
  #clean age to remove aged over 100 entries and make them NA first
  output$AGE <- ifelse(output$AGE >=100, NA, output$AGE)
  #changing the NAs to now become the median age, which is rounded to 35
  output$AGE[is.na(output$AGE)] <- round(mean(output$AGE[!is.na(output$AGE)]))


```


## Modeling
1. xgboost,
2. glmnet,
3. Avg NN

### Extreme Gradient Boosting


```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     # sampling = 'down',
                     savePredictions = T
                     )
xgbGrid <- expand.grid(
    nrounds=seq(14,24,2),
    max_depth=seq(2,8,2),
    eta=c(0.1, 0.2, 0.3),
    gamma=1,
    colsample_bytree=1,
    min_child_weight=1,
    subsample=1
)
xgbFit <- train(
    default~.,
    train.imp,
    method = 'xgbTree',
    trControl = ctrl,
    tuneGrid = xgbGrid
)

save(xgbFit, file = 'xgbFit')

saveRDS(xgbFit, file = 'xgbFit.rds')
```

```{r}

xgbFit <- readRDS("xgbFit.rds")
```



```{r}
print(xgbFit, details=F)

```

```{r}
plot(xgbFit)

xgb.importance(feature_names = colnames(train.imp),
               model = xgbFit$finalModel) %>%
  xgb.ggplot.importance() #gotta fix this.

densityplot(xgbFit,pch='|')
predict(xgbFit,type = 'prob') -> train.Probs
histogram(~Y + N, train.Probs)
```

## Elastinet

mixture of ridge & lasso


ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = 'down'
                     )
glmnetGrid <- expand.grid(.alpha = c(0,.2,.4,.6,.8,1),
                          .lambda = seq(10^-10,10^-1,0.02))
glmnetFit <- train(
    default~.,
    train.imp,
    trControl=ctrl,
    method='glmnet',
    tuneGrid = glmnetGrid
)

saveRDS(knnFit, file = 'glmnetFit.rds')


```{r}
glmnetFit <- readRDS("glmnetFit.rds")
```



```{r}
glmnetFit

```

```{r, glmnet}
densityplot(glmnetFit, pch='|')
plot(varImp(glmnetFit),15,main='Elastinet Model')
predict(glmnetFit, type = 'prob') -> train.glmnet.Probs
histogram(~Y + N, train.glmnet.Probs)
```

## K-NN
Supposed to be the worst.


ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = 'down'
                     )
knnGrid <- expand.grid(k=seq(3,23,2))
knnFit <- train(
    default~.,
    train.imp,
    method = 'knn',
    trControl = ctrl,
    tuneGrid = knnGrid
)

saveRDS(knnFit, file = 'knnFit.rds')

```{r}

knnFit <- readRDS("knnFit.rds")
```


```{r}
knnFit

```

```{r}
plot(knnFit)
densityplot(knnFit,pch='|')
predict(knnFit, type = 'prob') -> train.Probs
histogram(~Y+N, train.Probs)
```

## SVM

Code Chunk below
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = 'down'
                     )
svmFit <- train(
    default~.,
    train.imp,
    method = 'svmRadial',
    trControl = ctrl,
    tuneGrid = expand.grid(C=c(0.05,0.1,0.2,0.3), sigma=c(0.001,0.005,0.01,0.015))
)

save(svmFit, file = 'svmFit')
saveRDS(svmFit, file = 'svmFit')
```{r}

readRDS("svmFit")
```

```{r}
svmFit
```

```{r}
plot(svmFit)
densityplot(svmFit, pch='|')
predict(svmFit, type = 'prob') -> train.Probs
histogram(~Y+N, train.Probs)
```

## Light GBM
Supposed to be good

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = 'down'
                     )
lgbmGrid <- expand.grid(k=seq(3,23,2))
knnFit <- train(
    default~.,
    train.imp,
    method = 'RLightGBM',
    trControl = ctrl,
    tuneGrid = knnGrid
)

saveRDS(knnFit, file = 'knnFit.rds')

```{r}

knnFit <- readRDS("knnFit.rds")
```


```{r}
knnFit

```

```{r}
plot(knnFit)
densityplot(knnFit,pch='|')
predict(knnFit, type = 'prob') -> train.Probs
histogram(~Y+N, train.Probs)
```

# Compare the Models

```{r}
re <-
  resamples(x = list(
    xgb = xgbFit,
    knn = knnFit,
    elastinet = glmnetFit,
    svm = svmFit
  ))

```

```{r}
dotplot(re)
bwplot(re)

```

```{r}
difValues <- diff(re)
dotplot(difValues)

```

# Test Set Evaluation

## Create test set

```{r}

test.imp <- test.raw

#lakghd
test.imp$SEX <- as.factor(test.imp$SEX)
test.imp$EDUCATION <- as.factor(test.imp$EDUCATION)
test.imp$MARRIAGE <- as.factor(test.imp$MARRIAGE)

map_int(test.imp,~sum(is.na(.x)))

round(prop.table(table(test.imp$default)),2)

test.imp <- test.imp[complete.cases(test.imp), ]
```

## Predict Results

```{r}
elastinetPred   <- predict(object = glmnetFit, newdata = test.imp)
xgbPred         <- predict(object = xgbFit,    newdata = test.imp)
knnPred         <- predict(object = knnFit,    newdata = test.imp)
svmPred         <- predict(object = svmFit,    newdata = test.imp)

```


```{r}
library(purrr)
test.imp$default <- 0
test.imp$default <- as.factor(test.imp$default)
test.imp$default <- factor(test.imp$default, 
                           levels =
                             levels(train.imp$default))

xtab <- table(xgbPred,test.imp$default)
xgbCM <- confusionMatrix(xtab)

xtab <- table(elastinetPred,test.imp$default)
elastinetCM <- caret::confusionMatrix(xtab)

xtab <- table(knnPred,test.imp$default)
knnCM <-caret::confusionMatrix(xtab)

xtab <- table(svmPred,test.imp$default)
svmCM <-caret::confusionMatrix(xtab)


CM_list <- list(xgbCM, elastinetCM, knnCM, svmCM)

compiled_results <- tibble(
    models = c('xgb','elastinet','knn','svm'),
    accuracy = map_dbl(CM_list,~.x$overall[1]),
    kappa = map_dbl(CM_list,~.x$overall[2]),
    sensitivity = map_dbl(CM_list,~.x$byClass[1]),
    specificity = map_dbl(CM_list,~.x$byClass[2]),
    F1 = map_dbl(CM_list,~.x$byClass[7])
)
compiled_results %>% arrange(accuracy,kappa)
```

```{r}
library(ggrepel)
dotplot(reorder(models,accuracy)~accuracy,compiled_results, main = 'Accuracy (Test Set Performance)')
ggplot(compiled_results, aes(F1, accuracy)) +
    geom_point(color = 'blue',shape=1) +
    geom_text_repel(aes(label = models),
                    box.padding=unit(1,'lines'),
                    max.iter=1e2,segment.size=.3,
                    force=1) +
    theme_bw()+
    labs(x='F1',y='kappa', title='Kappa vs F1 (Test Set Performance)')

```


# Validation Set

***
```{r}
val.imp <- val.raw

#lakghd
val.imp$SEX <- as.factor(val.imp$SEX)
val.imp$EDUCATION <- as.factor(val.imp$EDUCATION)
val.imp$MARRIAGE <- as.factor(val.imp$MARRIAGE)

map_int(val.imp,~sum(is.na(.x)))

round(prop.table(table(test.imp$default)),2)

test.imp <- test.imp[complete.cases(test.imp), ]
```

```{r}
val.imp$default <- predict(xgbFit, val.imp)

final.output <- val.imp %>%
    select(ID, default)

final.output$default <- as.character(final.output$default)

final.output$default[final.output$default == "N"] = 0 #Set N to 0 and Y to 1
final.output$default[final.output$default == "Y"] = 1 #Set Y to 1

training.clean$default <- as.factor(training.clean$default)

write.csv(final.output, file="xgb_resultsEd.csv", row.names=FALSE)


```
