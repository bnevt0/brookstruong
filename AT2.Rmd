---
title: "AT2 Credit Default"
author: "Edward Truong / Alex Brooks"
date: "26/09/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objectives
1. Develop and deploy a classification model an a product purchase data set.
2. End to end analysis using R
3. Learn the `caret` package for ML
4. Learn to present the case using Rmarkdown

## Read in the dataset

```{r Read in the dataset, error=FALSE, message=FALSE, warning = FALSE}
library(tidyverse, quietly = T)
library(dplyr, quietly = T)      #used by Caret
library(ggplot2, quietly = T)
library(corrplot, quietly =T)
library(caret, quietly = T)
library(Amelia, quietly = T)
library(gridExtra, quietly = T)
library(xgboost, quietly = T)
library(ROCR, quietly = T)
library(Matrix, quietly = T)
library(rpart.plot, quietly =T)

setwd("~/Documents/brookstruong")

#Import the data
training.raw <- read_csv('AT2_credit_train_STUDENT.csv')
predict.raw <- read_csv('AT2_credit_test_STUDENT.csv')

```

23,101 observations over 17 variables for the training/test split
6899 observations over 17 variables for the prediction set.

##Clean the data based on new observations to make train and test the same

```{r data_clean}
#Data cleaning function we can use each time to make sure we share the same dataset  
#Cleaned ID into character, Sex into two variables (to remove animals), Age into NAs for over 100 errored data and then transformed those NAs into median age of 35, Education into 4 categorised variables, marriage into 3 categorised variables. See explanation in markdown if you want to know more.
#also removed previously coded factors of marriage, sex and education

clean_data <- function(dataSet) {
  
  #clean up sex to remove the animal classifications 
  output <- dataSet
  
  output$ID <- as.character(output$ID)
  
  output$SEX <- as.integer(output$SEX)
  
  output$SEX[output$SEX > 2] <- 0

   #all education factors greater than 4, set them to 4, also set 0 to 4
  output$EDUCATION[output$EDUCATION > 4] <- 4
  output$EDUCATION[output$EDUCATION == 0] <- 4
  #clean marriage
   output$MARRIAGE[output$MARRIAGE == 0] <- 3

  
  return(output)
}

```



```{r Trainset, echo=TRUE}
glimpse(training.raw)

training.raw$default <- as.factor(training.raw$default)
training.raw$SEX <- as.factor(training.raw$SEX)
training.raw$EDUCATION <- as.factor(training.raw$EDUCATION)
training.raw$MARRIAGE <- as.factor(training.raw$MARRIAGE)
training.raw$AGE <- as.numeric(training.raw$AGE)
#remove random anomilies

#remove NA
training.raw <- training.raw[complete.cases(training.raw), ]

#set negative balance values to 0

training.raw$LIMIT_BAL[training.raw$LIMIT_BAL <= 0] <- 0
training.raw$LIMIT_BAL[training.raw$LIMIT_BAL >800000] <-140000 

   #all education factors greater than 4, set them to 4, also set 0 to 4
  training.raw$EDUCATION[training.raw$EDUCATION == 5] <- 4
  training.raw$EDUCATION[training.raw$EDUCATION == 6] <- 4
  training.raw$EDUCATION[training.raw$EDUCATION == 0] <- 4
  training.raw$EDUCATION <- factor(training.raw$EDUCATION)
  training.raw$MARRIAGE <- factor(training.raw$MARRIAGE)
  #clean marriage
   training.raw$MARRIAGE[training.raw$MARRIAGE == 0] <- 3

#lets label our factors properly
   
training.raw$SEX <- factor(training.raw$SEX,
                           levels =c('1', '2'),
                           labels =c('Male', 'Female'))
training.raw$EDUCATION <- factor(training.raw$EDUCATION,
                                 levels =c('1', '2', '3', '4'),
                                 labels = c('Grad_School', 'Uni', 'High_School', 'Other'))
training.raw$MARRIAGE <- factor(training.raw$MARRIAGE,
                                levels = c('1','2','3'),
                                labels = c('Married', 'Single', 'Other'))

summary(training.raw)

```

```{r Correlation Plot}
cplot <- training.raw %>%
  select(-default) %>%
  select_if(is.numeric)

M <- cor(cplot)
p.mat <- cor.mtest(M)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, 
         method = "color",
         order= "hclust",
         type="full",
         col=col(200),
         diag =F,
         title="Correlation of Numeric Variables",
         addCoef.col = "black",
         sig.level = 0.05,
         insig ="blank",
         mar=c(0,0,3,0))

```

## Fix age imputation

```{r}

 training.raw$AGE <- ifelse(training.raw$AGE >=100, NA, training.raw$AGE)

map_int(training.raw,~sum(is.na(.x)))

'we can see that there is 50 age that are missing, out of the scheme of things, it is <1% of the total dataset, however lets play with using rpart to predict the missing age'


```

### Missing Values Imputation
```{r, }

train.imp <- training.raw[-1]

map_int(train.imp,~sum(is.na(.x)))

head(train.imp)
```

#### Age Imputation.

```{r}

age.predictors <- train.imp %>%
  select(LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_PC1) %>%
  filter(complete.cases(.))
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5)
rpartGrid <- data.frame(maxdepth = seq(2,10,1))
rpartFit_ageimputation <- train (x=age.predictors[,-3],
                                 y=age.predictors$AGE,
                                 method='rpart2',
                                 trControl = ctrl,
                                 tuneGrid = rpartGrid)
rpartFit_ageimputation

```

```{r}
plot(rpartFit_ageimputation)

# Tree depth of 7 is optimal

rpart.plot(rpartFit_ageimputation$finalModel)

save(rpartFit_ageimputation, file = 'rpartFit_ageimputation')
saveRDS(rpartFit_ageimputation, file = 'rpartFit_ageimputation.rds')
```

```{r}
missing_age <- is.na(train.imp$AGE)
age.predicted <- predict(rpartFit_ageimputation, newdata = train.imp[missing_age,])

train.imp[missing_age, 'AGE'] <- age.predicted

summary(train.imp)

```

## Train-Test Split

```{r Train-Test Split, echo=TRUE}
set.seed(42)
training_rows <- createDataPartition(y = train.imp$default, p=0.7, list=F)

test.imp <- train.imp %>% filter(!(rownames(.) %in% training_rows))
train.imp <- train.imp %>% filter(rownames(.) %in% training_rows)
dim(train.imp)

```

```{r pressure, echo=FALSE}
dim(test.imp)
```

## Missing values analysis

```{r}

map_int(train.imp,~sum(is.na(.x)))


```


## EDA

`default` is the response variable. The response variable is a Yes/No boolean variable therefor is appropriate for our classification problem. 

```{r}

round(prop.table(table(train.imp$default)),2)


```

76% of the dataset do not default and 24% have defaulted.

***

## Predictor Variables

### Univariate & Bivariate

First step is to look at all variables available using the `ggplot2` framework for visuals.

#### Continuous Variables

1. `LIMIT_BAL` The limit Balance beings at -99 with a median of 140,000, mean of 167880 and max of 1,000,000.
2. `AGE` certainly shows many outliers beyond the 100+ range. Age begins at 21 with a median of 34, mean of 35.65 and max of 141.

```{r}
library(scales)
p1 <- ggplot(data=train.imp, aes(x=LIMIT_BAL)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip() +
  scale_x_continuous(labels=comma)
p2 <- ggplot(data=train.imp, aes(x=AGE)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
grid.arrange(p1,p2, nrow=1)
```

```{r}

ggplot(train.imp,aes(x=L,y= ..count.. / sum(..count..))) +
  geom_jitter(aes(colour = SEX)) +
  theme_bw() +
  scale_shape (solid = F)

ggplot(data=train.imp, aes(x=LIMIT_BAL, y=AGE)) +
  geom_jitter(aes(fill=default)) +
  coord_flip()

```

```{r}
p3 <- ggplot(data=train.imp, aes(x=PAY_PC1)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p4 <- ggplot(data=train.imp, aes(x=PAY_PC2)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p5 <- ggplot(data=train.imp, aes(x=PAY_PC3)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
grid.arrange(p3,p4,p5, nrow=1)
```


```{r}
p6 <- ggplot(data=train.imp, aes(x=AMT_PC1)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p7 <- ggplot(data=train.imp, aes(x=AMT_PC2)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p8 <- ggplot(data=train.imp, aes(x=AMT_PC3)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p9 <- ggplot(data=train.imp, aes(x=AMT_PC4)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p10 <- ggplot(data=train.imp, aes(x=AMT_PC5)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p11 <- ggplot(data=train.imp, aes(x=AMT_PC6)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
p12 <- ggplot(data=train.imp, aes(x=AMT_PC7)) +
  geom_histogram(aes(fill=default), bins = 40) +
  coord_flip()
grid.arrange(p6,p7,p8,p9,p10,p11,p12, nrow=2)
```
#### Categorical Variables

1. `SEX` The limit Balance beings at -99 with a median of 140,000, mean of 167880 and max of 1,000,000.
2. `EDUCATION` certainly shows many outliers beyond the 100+ range. Age begins at 21 with a median of 34, mean of 35.65 and max of 141.
2. `MARRIAGE` certainly shows many outliers beyond the 100+ range. Age begins at 21 with a median of 34, mean of 35.65 and max of 141.

```{r}

get_legend<-function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
p <- lapply(X = c('SEX', 'EDUCATION', 'MARRIAGE'),
            FUN = function(x) ggplot(data = train.imp) +
              aes_string(x=x, fill = 'default') +
              geom_bar(position="dodge") +
              theme(legend.position="none"))
legend <- get_legend(ggplot(data = train.imp, aes(x=SEX, fill = default)) +
                       geom_bar())

grid.arrange(p[[1]],p[[2]],p[[3]],
             legend, layout_matrix = cbind(c(1,2,3),
                                           c(4,5,3),
                                           c(6,6,6)),
             widths=c(3,3,1))

```

***
## Data Preparation

## Modeling
1. xgboost,
2. glmnet,
3. Avg NN

### Extreme Gradient Boosting


```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     # sampling = 'down',
                     savePredictions = T
                     )
xgbGrid <- expand.grid(
    nrounds=seq(14,24,2),
    max_depth=seq(2,8,2),
    eta=c(0.1, 0.2, 0.3),
    gamma=1,
    colsample_bytree=1,
    min_child_weight=1,
    subsample=1
)
xgbFit <- train(
    default~.,
    train.imp,
    method = 'xgbTree',
    trControl = ctrl,
    tuneGrid = xgbGrid
)

saveRDS(xgbFit, file = 'xgbFit.rds')
```


```{r}
print(xgbFit, details=F)

```

```{r}
plot(xgbFit)

xgb.importance(feature_names = colnames(train.imp),
               model = xgbFit$finalModel) %>%
  xgb.ggplot.importance() #gotta fix this.

varImp(xgbFit)

densityplot(xgbFit,pch='|')
predict(xgbFit,type = 'prob') -> train.Probs
histogram(~Y + N, train.Probs)
```

## reduced features

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     sampling = 'smote',
                     savePredictions = T
                     )
xgbGrid <- expand.grid(
    nrounds=seq(14,24,2),
    max_depth=seq(2,8,2),
    eta=c(0.1, 0.2, 0.3),
    gamma=1,
    colsample_bytree=1,
    min_child_weight=1,
    subsample=1
)
xgbFitF <- train(
    default~
      PAY_PC1 +
      AGE +
      LIMIT_BAL +
      AMT_PC1 +
      EDUCATION +
      PAY_PC2 +
      PAY_PC3 +
      AMT_PC2 +
      AMT_PC7,
    train.imp,
    method = 'xgbTree',
    trControl = ctrl,
    tuneGrid = xgbGrid,
    metric = "ROC"
)

saveRDS(xgbFitF, file = 'xgbFitF.rds')
```


```{r}
print(xgbFitF, details=F)

```

```{r}
plot(xgbFitF)

xgb.importance(feature_names = colnames(train.imp),
               model = xgbFit$finalModel) %>%
  xgb.ggplot.importance() #gotta fix this.

varImp(xgbFitF)

densityplot(xgbFitF,pch='|')
predict(xgbFit,type = 'prob') -> train.ProbsF
histogram(~Y + N, train.ProbsF)
```

## Elastinet

mixture of ridge & lasso


ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = 'down'
                     )
glmnetGrid <- expand.grid(.alpha = c(0,.2,.4,.6,.8,1),
                          .lambda = seq(10^-10,10^-1,0.02))
glmnetFit <- train(
    default~.,
    train.imp,
    trControl=ctrl,
    method='glmnet',
    tuneGrid = glmnetGrid
)

saveRDS(knnFit, file = 'glmnetFit.rds')


```{r}
glmnetFit <- readRDS("glmnetFit.rds")
```



```{r}
glmnetFit

```

```{r, glmnet}
densityplot(glmnetFit, pch='|')
plot(varImp(glmnetFit),15,main='Elastinet Model')
predict(glmnetFit, type = 'prob') -> train.glmnet.Probs
histogram(~Y + N, train.glmnet.Probs)
```

## K-NN
Supposed to be the worst.


ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = 'down'
                     )
knnGrid <- expand.grid(k=seq(3,23,2))
knnFit <- train(
    default~.,
    train.imp,
    method = 'knn',
    trControl = ctrl,
    tuneGrid = knnGrid
)

saveRDS(knnFit, file = 'knnFit.rds')

```{r}

knnFit <- readRDS("knnFit.rds")
```


```{r}
knnFit

```

```{r}
plot(knnFit)
densityplot(knnFit,pch='|')
predict(knnFit, type = 'prob') -> train.Probs
histogram(~Y+N, train.Probs)
```

## SVM

Code Chunk below
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = 'down'
                     )
svmFit <- train(
    default~.,
    train.imp,
    method = 'svmRadial',
    trControl = ctrl,
    tuneGrid = expand.grid(C=c(0.05,0.1,0.2,0.3), sigma=c(0.001,0.005,0.01,0.015))
)

save(svmFit, file = 'svmFit')
saveRDS(svmFit, file = 'svmFit')
```{r}

readRDS("svmFit")
```

```{r}
svmFit
```

```{r}
plot(svmFit)
densityplot(svmFit, pch='|')
predict(svmFit, type = 'prob') -> train.Probs
histogram(~Y+N, train.Probs)
```

## Light GBM
Supposed to be good

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     verboseIter = F,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = T
                     # sampling = 'down'
                     )
lgbmGrid <- expand.grid(k=seq(3,23,2))
knnFit <- train(
    default~.,
    train.imp,
    method = 'RLightGBM',
    trControl = ctrl,
    tuneGrid = knnGrid
)

saveRDS(knnFit, file = 'knnFit.rds')

```{r}

knnFit <- readRDS("knnFit.rds")
```


```{r}
knnFit

```

```{r}
plot(knnFit)
densityplot(knnFit,pch='|')
predict(knnFit, type = 'prob') -> train.Probs
histogram(~Y+N, train.Probs)
```

# Compare the Models

```{r}
re <-
  resamples(x = list(
    xgb = xgbFit,
    knn = knnFit,
    elastinet = glmnetFit,
    svm = svmFit
  ))

```

```{r}
dotplot(re)
bwplot(re)

```

```{r}
difValues <- diff(re)
dotplot(difValues)

```

# Test Set Evaluation

## Create test set

```{r, Clean test set}

test.imp

summary(test.imp)

map_int(test.imp,~sum(is.na(.x)))


test.imp <- test.imp[complete.cases(test.imp), ]
```

## Predict Results

```{r}

elastinetPred   <- predict(object = glmnetFit, newdata = test.imp)
xgbPred         <- predict(object = xgbFit,    newdata = test.imp)
xgbPredSmoteF         <- predict(object = xgbFitF,    newdata = test.imp)
knnPred         <- predict(object = knnFit,    newdata = test.imp)
svmPred         <- predict(object = svmFit,    newdata = test.imp)

```


```{r}
library(purrr)
test.imp$default <- 0
test.imp$default <- as.factor(test.imp$default)
test.imp$default <- factor(test.imp$default, 
                           levels =
                             levels(train.imp$default))

xtab <- table(xgbPred,test.imp$default)
xgbCM <- confusionMatrix(xtab)

xtab <- table(xgbPredSmoteF,test.imp$default)
xgbCM <- confusionMatrix(xtab)

xtab <- table(elastinetPred,test.imp$default)
elastinetCM <- caret::confusionMatrix(xtab)

xtab <- table(knnPred,test.imp$default)
knnCM <-caret::confusionMatrix(xtab)

xtab <- table(svmPred,test.imp$default)
svmCM <-caret::confusionMatrix(xtab)


CM_list <- list(xgbCM, elastinetCM, knnCM, svmCM)

compiled_results <- tibble(
    models = c('xgb','elastinet','knn','svm'),
    accuracy = map_dbl(CM_list,~.x$overall[1]),
    kappa = map_dbl(CM_list,~.x$overall[2]),
    sensitivity = map_dbl(CM_list,~.x$byClass[1]),
    specificity = map_dbl(CM_list,~.x$byClass[2]),
    F1 = map_dbl(CM_list,~.x$byClass[7])
)
compiled_results %>% arrange(accuracy,kappa)
```

```{r}
library(ggrepel)
dotplot(reorder(models,accuracy)~accuracy,compiled_results, main = 'Accuracy (Test Set Performance)')
ggplot(compiled_results, aes(F1, accuracy)) +
    geom_point(color = 'blue',shape=1) +
    geom_text_repel(aes(label = models),
                    box.padding=unit(1,'lines'),
                    max.iter=1e2,segment.size=.3,
                    force=1) +
    theme_bw()+
    labs(x='F1',y='kappa', title='Kappa vs F1 (Test Set Performance)')

```


# Validation Set

***
```{r}
predict.raw <- read_csv('AT2_credit_test_STUDENT.csv')
val.imp <- predict.raw

val.imp$SEX <- as.factor(val.imp$SEX)
val.imp$SEX <- factor(val.imp$SEX,
                           levels =c('1', '2'),
                           labels =c('Male', 'Female'))
#remove random anomilies
#set negative balance values to 0


   #all education factors greater than 4, set them to 4, also set 0 to 4
  val.imp$EDUCATION[val.imp$EDUCATION == 5] <- 4
  val.imp$EDUCATION[val.imp$EDUCATION == 6] <- 4
  val.imp$EDUCATION[val.imp$EDUCATION == 0] <- 4
  val.imp$EDUCATION <- factor(val.imp$EDUCATION,
                                 levels =c('1', '2', '3', '4'),
                                 labels = c('Grad_School', 'Uni', 'High_School', 'Other'))

  #clean marriage
val.imp$MARRIAGE[val.imp$MARRIAGE == 0] <- 3
val.imp$MARRIAGE <- as.factor(val.imp$MARRIAGE)
val.imp$MARRIAGE <- factor(val.imp$MARRIAGE,
                                levels = c('1','2','3'),
                                labels = c('Married', 'Single', 'Other'))
#lets label our factors properly

```

```{r}
val.imp$default <- predict(xgbFitF, val.imp)

final.output <- val.imp %>%
    select(ID, default)

final.output$default <- as.character(final.output$default)

final.output$default[final.output$default == "N"] = 0 #Set N to 0 and Y to 1
final.output$default[final.output$default == "Y"] = 1 #Set Y to 1

training.clean$default <- as.factor(training.clean$default)

final.output

write.csv(final.output, file="xgb_resultsEdSMOTEV.csv", row.names=FALSE)


```
